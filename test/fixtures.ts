export const blogPost = `
"When multiple processes run at the same time, they are said to run concurrently. When multiple processes share data, a number of problems can arise.\n\nConsider a scenario where one process is computing the sum of a user's order history while at the same time another process is mutating the same user's order history. If the data comprising the user's order history is not protected from mutation, then the sum computed by one process can be inaccurate relative to the final outcome of both processes.\n\nTraditional relational databases provide isolation through commitment control concepts like transactions.\n\nTransactions in traditional relational databases involve high-level application decisions such as:\n\nBeginning a transaction;\nPerforming work like inserting, updating, and deleting rows or tables;\nEnding a transaction with the implicit or explicit option to commit or rollback.\n\nDuring the course of a transaction, traditional relational databases implement isolation by locking the underlying resources. Depending on the isolation level chosen, the affected rows and tables are locked so that two concurrent processes cannot interfere with each other.\n\nIn the case of DynamoDB, commitment control is not provided. The standard offering in DynamoDB does not provide locks at either the row-level or table-level.\n\nThis article will discuss transactions in DynamoDB, their limitations, and a proposed solution involving distributing locking, replete with details.\n\nTransactions in DynamoDB\n\nDynamoDB is the AWS defacto NoSQL solution. DynamoDB provides great flexibility for storing a wide range of data with impressive scaling and high throughput. DynamoDB also provides ACID (atomicity, consistency, isolation, and durability) guarantees through transactions.\n\nAlthough DynamoDB transactions have many benefits there are some limitations. While transactions provide some guarantees surrounding atomicity and consistency, the solution does not provide units of work. Additionally, isolation is only provided through interference detection.\n\nA transaction in DynamoDB is an all-or-nothing operation without any residual locking, committing, or rollback operations. Transactions fail in DynamoDB if two processes interfere with any shared data. Either one or both of the failed transactions are rolled-back entirely.\n\nAs a consequence, a common strategy for transaction fault-tolerance when working with DynamoDB is retry with exponential back-off. While this rudimentary approach may work for common use cases, applications which might otherwise take advantage of isolation levels and units of work become complicated with loops and additional state-management.\n\nFinally, transactions in DynamoDB have severe limitations. The number of rows in a DynamoDB transaction is limited to 25. Additionally, no row can be referenced by more than one check or mutation in a single transaction. This means you cannot apply a condition and also update a row, for example.\n\nDistributed Locking\n\nDistributed locking is a technique which, when implemented correctly, guarantees that two processes cannot access shared data at the same time. The processes rely upon the locking protocol to ensure that only one is allowed to proceed once a lock is acquired.\n\nCommon implementations for distributed locking typically involve a lock manager that processes communicate with. The lock manager controls the state of the locks.\n\nImplementing a distributed lock is difficult to get right because there are edge cases to consider such as:\n\nRace conditions - multiple processes acquiring the lock simultaneously\nDeadlocks - multiple processes competing for separate locks out of order\nStale locks - a lock which is granted but no longer valid\n\nOne naive solution for a distributed lock involves a record of the current lock holder while competing processes repeatedly try to be the next lock holder. Typically this entails retry with exponential back-off.\n\nOne drawback of this approach is that a process could be very unlucky while other processes are very lucky. The first process which cannot hold the lock sleeps for a period of time, only to find upon waking that another process is holding the lock. The process sleeps for a longer period of time and again awakes to find yet another process holding the lock. Theoretically, this can continue forever, resulting in the process suffering from resource starvation. The process is unable to obtain the lock while other processes were able to complete their work — even out of order.\n\nThe aforementioned solution is discussed in this implementation provided by engineers at AWS:\n\nBuilding Distributed Locks with the DynamoDB Lock Client | Amazon Web Services\n\nThe solution proposed in this article overcomes this limitation through a ticket-based queuing mechanism.\n\nImplementing Concurrency Locks in DynamoDB\n\nConsider the scenario where two or more processes may interfere with each other. One process might try to insert, update, or delete one or more rows. If there is no other active process, then there is no problem because there is zero concurrency. Without concerns of concurrency, isolation levels and units of work have no real value. In reality, a robust application must assume there are potentially many concurrent processes vying for the same data.\n\nWithout full commitment control features like units of work, commit, rollback, and row-level locking, any sufficiently complex transaction can easily exceed the limitations of DynamoDB transactions.\n\nSince DynamoDB provides neither isolation levels nor units of work, and transactions are severely limited in their size and capability, applications need another way to isolate themselves from each other. If applications can logically isolate each other from accessing shared data, they can be assured they will not interfere with each other.\n\nAn ideal solution will have the following characteristics:\n\nFIFO (first-in, first-out) - access to shared data will be handled in the order requested\nIsolated - no two processes can share the data concurrently\nRecord wait - a process can choose to wait for a defined amount of time before failing\nFault-tolerant - no user or application intervention will be required to handle failures\nSelf-cleaning - any locks acquired must be removed automatically\nQueueing\n\nImagine a set of processes as a FIFO (first-in, first-out) queue. The first such process will have no competing process; naturally it is the first and therefore is able to perform its work. Subsequent processes must wait their respective turns before accessing the shared data. If each process were to \"take a number\" so to speak, then after the first process completes its work, the process next in line will be allowed to proceed.\n\nDynamoDB has a feature which can be used to create such a mechanism: atomic counters. An atomic counter in DynamoDB allows an application to update a row through incrementing the value. While the order of simultaneous processes accessing the counter is not guaranteed, the result is that no two processes will obtain the exact same result. Additionally, the value is atomically incremented and globally unique.\n\nNote DynamoDB supports integer values with up to 38 digits of precision. This means a ticket number can range between 1 and 99,999,999,999,999,999,999,999,999,999,999,999,999. That is 99 undecillion ticket numbers. Using this scheme, processes requesting one million ticket numbers per second every day, every year, would take 31,536,000,000,000 years before the limit is reached.\n\nThe row holding the ticket number will need a common partition key value known to all processes. If the ticket number is shared across multiple disparate locks, then a simple name will suffice. If namespacing is desirable, then a more complex partition key with prefixes or a partition key combined with a sort key namespace is a possibility.\n\nExample atomic counter increment\n\n/**\n * In this example, assume the DynamoDB table 'my-dynamodb-table' has a composite key: pk, sk\n * where pk (partition key) and sk (sort key) are both string values.\n */\n\nimport * as AWS from 'aws-sdk';\n\nconst client = new AWS.DynamoDB.DocumentClient({ accessKeyId, endpoint, secretAccessKey });\n\nconst key: Record<string, string> = {\n  pk: 'ticket-master',\n  sk: 'current'\n}\n\nlet ticketNumber: number | BigInt = 1;\n({\n  Attributes: { ticketNumber },\n} = await client\n  .update({\n    ConditionExpression: 'attribute_exists(pk)',\n    ExpressionAttributeValues: {\n      ':ticketNumber': ticketNumber\n    },\n    Key: key,\n    ReturnValues: 'UPDATED_NEW',\n    TableName: 'my-dynamodb-table',\n    UpdateExpression: 'SET ticketNumber = ticketNumber + :ticketNumber',\n  })\n  .promise()\n);\n\ntypescriptCopy to clipboard\n\nThere is an edge-case here: what if the row does not yet exist? In this case the ConditionExpression above will cause the update to fail. To account for this, the process should trap for the failure and seed the initial value.\n\nawait client\n  .put({\n    ConditionExpression: 'attribute_not_exists(pk)',\n    Item: {\n      ...key,\n      ticketNumber\n    },\n    TableName: 'my-dynamodb-table',\n  })\n  .promise();\n\ntypescriptCopy to clipboard\n\nThere is yet another edge-case here: what if another competing process beats the first process in seeding the initial value? In this case the ConditionExpression here will fail also. The solution is to retry the first update statement a second time. One of these two operations will ultimately succeed.\n\nOnce the ticket number is determined, the process enters the queue by creating a row in DynamoDB where the sort key defines the order of the data using the ticket number. The partition key of the queue row is the name of the entity being locked. If multiple types of locks are desired against the same entity, then the sort key can implement a namespace prefix followed by the ticket number.\n\nExample creating queued lock row\n\n/**\n * In this example, the pk (partition key) is the name of the entity which is to be locked.\n * The sk (sort key) implements a namespacing strategy, which allows for multiple, independent\n * locks to be applied to the same entity.\n * To prevent deadlocks, any two processes would need to use the same ticket number for each lock.\n */\n\nconst now = Date.now() / 1000;\n\nawait client\n  .put({\n    ConditionExpression: 'attribute_not_exists(pk)',\n    Item: {\n      createdAt: now,\n      expiresAt: now + 60,\n      pk: 'identity-of-locked-entity',\n      sk: \`locked-for-some-reason/\${ticketNumber.toString().padStart(38, '0')}\`\n    },\n    TableName: 'my-dynamodb-table',\n  })\n  .promise();\n\ntypescriptCopy to clipboard\n\nNote If the sort key is not numeric, then for proper hexadecimal sorting to work the ticket number must be left-padded with zeros.\n\nIsolation\n\nTo isolate any two processes each process will need to check their relative position in the queue. Once a given process is the first in the queue then it is allowed to proceed. This is accomplished by querying the queue rows.\n\nThe query operation will specify the partition key, which is the name of the entity locked. If lock namespaces are implemented, the sort key can use the begins_with operator where the prefix is the namespace. Otherwise, the sort key is the ticket number and the sort key component of the key expression can request those rows whose ticket number is less than the ticket number obtained by the given process.\n\nFor the rows that are queried, the process determines if the sort key matches the sort key containing the ticket number for the relevant process. If it does not and it is less than the ticket number for the given process, then that process is still waiting in the queue. If the sort key does match and there are no processes before the given process, then that process is first in the queue and is allowed to proceed.\n\nExample checking queue position\n\n/**\n * In this example, the process iterates through pages of locks.\n * Expired locks are captured for subsequent removal.\n */\n\nconst expiredLocks: Record<string, any>[] = [];\nconst now = Date.now() / 1000;\n\nlet page: Record<string, any>[] = [];\nlet resumeAfter: Record<string, any> | undefined;\n\ndo {\n  ({ Items: page = [], LastEvaluatedKey: resumeAfter } = await client\n    .query({\n      ConsistentRead: true,\n      ExclusiveStartKey: resumeAfter,\n      ExpressionAttributeValues: {\n        pk: 'identity-of-locked-entity',\n        sk: 'locked-for-some-reason/',\n      },\n      KeyConditionExpression: 'pk = :pk AND begins_with(sk, :sk)',\n      Limit: 100,\n      ScanIndexForward: true,\n      TableName: 'my-dynamodb-table',\n    })\n    .promise()\n  );\n\n  for (const entry of page) {\n    if (entry.expiresAt < now) {\n      // capture expired locks for removal\n      expiredLocks.push(entry);\n    } else if (entry.sk === sk) {\n      // expired locks can be proactively deleted\n      // await Promise.all(expiredLocks.map( ... see below ... ));\n\n      // reached the front of queue\n      return true;\n    } else {\n      // not at front of queue; cannot proceed\n      return false;\n    }\n  }\n} while (resumeAfter);\n\ntypescriptCopy to clipboard\n\nNote It is important that strongly consistent reads are enforced when checking the queue. This ensures that all pending updates to the partition key are applied before reading the queue.\n\nSince DynamoDB implements pagination for queries, the process needs to iterate through each page of locks. In each page, the process examines each returned lock to determine its relative position.\n\nIf the first row of the first page has the matching sort key containing the ticket number obtained by the given process, then the process is the first in line and should proceed with its work.\n\nIf the process is not first in the queue, then the process should sleep for a short period of time and re-check the queue. The polling interval should be fairly short, perhaps 500 milliseconds.\n\nIf the process implements proactive deletion of expired locks, each row returned should be deleted if the expiration time is less than the current time.\n\nAfter completing its work, the process which was first in the queue needs to delete its queue entry.\n\nExample deleting queued lock row\n\nawait client\n  .delete({\n    Key: {\n      pk: 'identity-of-locked-entity',\n      sk: \`locked-for-some-reason/\${ticketNumber.toString().padStart(38, '0')}\`,\n    },\n    TableName: 'my-dynamodb-table',\n  })\n  .promise();\n\ntypescriptCopy to clipboard\nRecord wait\n\nProcesses should not check the queue indefinitely — unless it chooses to do so. During queue checking the process can check the elapsed time against a start time. Once the configured wait time for a lock has been reached the process can choose to throw an exception or otherwise return unsuccessfully.\n\nIf the record wait parameter is 0, then the process should fail if there is any other process already in the queue. This is known as no wait.\n\nIf the record wait parameter is Infinity, then the process should never stop waiting to reach the front of the queue.\n\nA commonly used setting for this parameter is sixty seconds.\n\nFault-tolerance\n\nWhen creating the queue row, an expiration timestamp is also included. Rows created in the queue cannot exist forever. In fact, rows should have a reasonably short time to live such as sixty seconds. When a given process creates its entry in the queue, it will need to periodically refresh the expiration time. If sixty seconds is chosen as the time for the lock to live, then a half-life value of thirty seconds is a reasonable interval of time for the process to refresh the lock.\n\nIn other words, the original lock entry in the queue defines an expiration of sixty seconds from the time of creation. Every thirty seconds later the process should update the expiration time to be another sixty seconds from then.\n\nExample lock heartbeat\n\nconst heartbeat: NodeJS.Timeout = setInterval(async () => {\n  await client\n    .update({\n      ConditionExpression: 'attribute_exists(pk)',\n      ExpressionAttributeValues: {\n        ':expiresAt': Date.now() / 1000 + 60,\n      },\n      Key: {\n        pk: 'identity-of-locked-entity',\n        sk: \`locked-for-some-reason/\${ticketNumber.toString().padStart(38, '0')}\`,\n      },\n      TableName: 'my-dynamodb-table',\n      UpdateExpression: 'SET expiresAt = :expiresAt',\n    })\n    .promise();\n}, 30000)\n\ntypescriptCopy to clipboard\n\nThis \"heartbeat\" approach to refreshing the lock expiration value, combined with a relatively short time to live, provides fault-tolerance. If a given process fails to refresh the expiration time before it expires, then it will shortly thereafter be considered stale. During the queue check, processes can ignore those rows which are considered expired. Such rows would have an expiration value which is less than the current time.\n\nNote Two servers may experience clock drift. A sixty-second time to live and a thirty-second refresh interval provides a generous thirty-second range of variance to account for such anomalies.\n\nImportant The heartbeat interval created needs to be cleared once the process completes its work. Otherwise, the process will hang indefinitely and continue to hold the lock, blocking all other processes waiting in the queue.\n\nSelf-cleaning\n\nRows which are found to be expired can either be ignored, proactively removed, or lazily removed by DynamoDB. If the rows are ignored, then stranded locks will grow over time, unnecessarily increasing the size of the table. Additionally, the time to process the queue to determine the relative position of a given process will increase over time.\n\nProcesses can proactively clean any expired rows by deleting them from the queue.\n\nAlternatively, the TTL (time to live) feature of DynamoDB can be used here. With the TTL feature, DynamoDB will periodically scan and delete expired items automatically.\n\nIf the TTL feature is used, then the expiration time needs to be a numeric value representing the time of expiration using the Unix epoch time format, in seconds. That is, the number of seconds elapsed since 1970-01-01T00:00:00.000Z, plus the amount of time to live, in seconds.\n\nConclusion\n\nThe capabilities of DynamoDB make it an attractive, all-purpose NoSQL solution. Nevertheless, the limitations of its transactions introduce complications that might cause developers to choose another database system. Specifically, the inability to choose to commit or rollback units of work is an important consideration.\n\nThe solution described here does not provide complete commitment control. The solution only helps to simplify application design through isolation. Implementing application-level concurrency ensures that no two processes can hold a lock on the resource(s) identified. Isolating processes from shared data helps in reasoning about the application. Specifically, isolation prevents applications from experiencing the side effects of lost updates, dirty reads, phantom reads, and non-repeatable reads."
`

export const multilineBlogPost = [
  "At Nearform, we're always on the lookout for emerging technologies that have the potential to wow our customers and transform their organisations. Just like how we became big supporters of Node.js in its early days, we're now diving into the realm of large language models (LLM) powered applications.\n\nNow, you might be wondering what LLM-powered applications are all about. Well, imagine applications like ChatGPT that use language models to generate responses. It's pretty awesome, right? But here's where it gets even more interesting — when we integrate these LLM agents with our existing tools, APIs and databases, it takes everything to the next level. We can build specialised tools where the LLM plays a crucial yet small part.\n\nThe goal of our latest project was to create our own version of the OpenAI Playground. It was a fantastic opportunity for us to dive deep into the code and see how much additional work we needed to do on top of the OpenAI API. How far could we push this? Plus, it laid a solid foundation for our teams to build future LLM-powered products.",
  "The Playground\n\nThe initial steps were straightforward. We used LangChain.js, a handy wrapper for the LLM API, and built a simple REST API using Fastify to expose the necessary parameters.\n\nNothing too complex... until we started working on supporting long conversations, fine-tuning responses to specific goals and integrating with external APIs and tools. That's when things got really interesting.\n\nBut before we delve deeper into the technicalities, let's take a quick tour of Nearform’s LLM Playground and its different components.\n\nStarting from the top bar, we built the ability to import and export all data in the playground. This feature has been a lifesaver when collaborating on refining prompts and testing out new ideas.\n\nOn the right sidebar, we have parameters that control how the LLM generates replies. These are parameters specific to the OpenAI API and we merely proxy them through.  Changing the model’s parameters can greatly impact its ability, performance and maximum conversation length. Then we have temperature and TopP, which determine the level of randomness in the replies, and penalties, which further refine the responses.\n\nIn the central section of the page, we have the message history. Each message plays a specific role — it can be a User asking a question, an Assistant replying, or even a special Function reply (we'll come back to that later). We can edit the full history and modify the replies provided by the Assistant. This feature is invaluable for reproducing edge cases without having to retry the conversation multiple times until we achieve a specific response.\n\nNow, let's focus on the leftmost section — the System. This is where we give instructions to the LLM Assistant about its goal and behaviour. It can be as simple as saying \"You are a helpful assistant,\" or we can get creative and ask it to reply in different languages or even pirate dialects (arr, matey!). But it doesn't stop there — we can demand structured data back from the model, specify JSON schemas and indicate the types of responses we're willing to accept. It's like having a genie that grants our data-related wishes.\n\nOne key limitation of most LLMs is that they don't have access to live data and lack awareness of the current time. If you need to consider temporal references in its responses, tough luck! But fear not, we have a solution: functions. Functions allow the LLM to interact with the outside world, perform searches on Google, access internal APIs and databases, or even ask for the current time. They provide the agent with more context and are the secret sauce behind the scenes. To keep things neat, we have a handy debug button to test the conversation without functions interfering.",
  'LangChain.js\n\nIn this project, we didn’t use the OpenAI client directly. Instead, we used LangChain, which in turn uses the OpenAI client. There are multiple LLM providers out there and at the core functionality they are very similar but they all have their own additional specific features.\n\nLangChain aims to be the library that normalises all these features and allows us to be (mostly) provider-agnostic when implementing an LLM app. Not only that, but LangChain is also a great learning tool when it comes to providing prompt engineering examples. To implement some missing features, LangChain leverages the LLMs themselves and through the use of creative prompts it backfills the features.',
  "Memory and long conversation chains\n\nNow, let's address an interesting challenge we encountered — long conversation chains. The OpenAI API has a maximum capacity for processing requests, and anything beyond that results in an error. Interestingly, capacity isn't measured in characters or bytes but in tokens. Tokens can vary in size depending on the content — for example, uppercase words consume more tokens than lowercase ones (capital letters be greedy!). In general, we can consider a token to be four characters in the English language.\n\nDifferent OpenAI models will have different limits, but it’s safe to say that the higher the limit the more expensive each API call will be. For example, at the time of writing, GPT-4 has a limit of 8,192 tokens whereas the much cheaper GTP-3 Turbo has a limit of only 4,096 tokens. This means that for any practical application, these limits are fairly quick to reach, leading to application errors.\n\nAt first, we simply showed the error to the user. Our next approach was to remove the oldest messages from the chain to make it work. However, this approach meant losing important context from those removed messages. So, we set out to find a better solution.\n\nLangChain implements the concept of memory. LangChain has multiple memory types but the one we found most interesting was the conversation summary buffer. This memory strategy keeps the most recent messages in full but summarises the older ones. This ensures we keep all the details and context for recent messages while still keeping some of the most relevant details on the old ones.\n\nLangChain doesn't rely on complex code to parse the message history and extract context. Instead, it cleverly uses the LLM itself in a parallel conversation thread. It sends the first few messages and asks the LLM to generate a compact conversation history with all relevant context. It’s the LLM itself that generates the summary.",
  "Assistants, Functions and Agents\n\nLLMs, or Language Models, have limitations when it comes to calling third-party APIs and accessing real-time data. They rely solely on the information they were trained on and cannot even determine the current time. However, there is a solution called Functions that can help overcome these challenges. Functions allow us to call third-party APIs when necessary.\n\nFor example, if you ask an LLM about the weather in London or Bristol, it will give a generic response stating that it doesn't have access to real-time data. But we can teach the LLM to recognise when it should call a weather API. Then, using our application code, we can make the API call ourselves and provide the LLM with the additional information it needs.\n\nThere are several challenges the LLM must overcome to implement functions:\n\nThe LLM must be aware that there is a weather API and has to be able to determine when it would be appropriate to call it\n\nThe LLM needs to provide structured data as a JSON object with the required parameters for the API so that we call it from our own server\n\nThe LLM must parse the API reply we provide and generate an appropriate reply with that information\n\nThis article explains how the Agent model can be implemented in only 100 lines of code.\n\nPreviously, LangChain supported this functionality through Agents and Tools, which was a creative approach. However, now OpenAI has optimised the process and their LLM API has built-in support for Functions and Assistants.\n\nFunctions aren't just useful for calling third-party APIs. They can be even more powerful. We can utilise them to answer questions with structured data or guide the conversation to collect the necessary information for our app workflow.\n\nLet's say we're using the LLM to support a user's onboarding process in a web application. We need to gather multiple data points from the user like their first name, surname, date of birth and email address. With Functions, we can define a schema for a user and instruct the LLM to use that function.\n\nNow, the LLM will keep the conversation going until it gathers all the required data points. It can generate appropriate questions to collect the missing information and review the conversation history to determine what's missing next. The real value lies in the LLM understanding the context and gathering the necessary data on its own.",
  "Streaming\n\nLLMs face a significant challenge — they tend to be quite slow in responding. It can take a few seconds for them to reply to most requests. However, here's the good news: LLMs don't need all that time to generate a complete reply. They actually analyse the entire context and generate one additional piece of information at a time. They keep doing this recursively until they sense it's time to stop.\n\nNow, let's talk about reading speed. On average, an adult can comfortably read around five words per second. However, even at a pace of four or three words per second, it still feels comfortable to read. It's only when we have to wait for a reply for a longer time that it becomes uncomfortable.\n\nThe cool part is that we can make use of this knowledge to enhance the conversation experience. Instead of delivering the entire reply at once, we can stream it a few tokens at a time. By doing this, we can make the interaction feel more like a real conversation.",
  "Conclusion\n\nThere you have it: the exciting world of the LLM Playground. It's a place where developers and business people alike can explore the power of large language models and how they can be harnessed to build amazing software.\n\nCreating this application allowed us to get a deeper understanding of how non-trivial aspects of LLMs work, and how frameworks such as LangChain are built.\n\nWe believe that nowadays it’s essential for companies to stay on top of the current AI trends, as the landscape is in constant and rapid evolution, and new approaches to solving problems are researched, discovered and published regularly.\n\nLLMs open up a great variety of opportunities for software creators and users alike, and keeping ourselves up to date with industry trends allows us to chase opportunities as they show up.",
  'Augmenting user experiences using LLMS: An analysis of our travel AI agent application\nANTOINE MARIN\n17 OCT 2023\nHow to Create Efficient Prompts for LLMs\nGREG DUCKWORTH\n5 JAN 2024'
]
